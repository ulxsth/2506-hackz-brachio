# Supabase Realtime スケーラビリティ分析 🚀

## 概要
TYPE 2 LIVE（リアルタイム100人規模のタイピングゲーム）におけるSupabase Realtimeの適性を調査しました。

## 制限・性能の調査結果 📊

### 同時接続数の制限
- **Free Plan**: 200同時接続
- **Pro Plan**: 500同時接続 (その後$10/1000接続)
- **Team Plan**: 500同時接続 (その後$10/1000接続)
- **Enterprise**: カスタム（相談）

### メッセージ容量・頻度
- **Free Plan**: 月間200万メッセージ
- **Pro Plan**: 月間500万メッセージ (その後$2.50/100万メッセージ)
- **最大メッセージサイズ**: 
  - Free: 250KB
  - Pro/Team: 3MB

### アーキテクチャ特性
- **技術**: Elixir + Phoenix Framework
- **WebSocket**: 高性能な接続管理
- **メッセージ配信**: 保証なし（ベストエフォート）
- **データ処理**: Broadcastは一時的、Presenceは計算集約的

## 100人規模のタイピングゲームでの適性分析 🎯

### ✅ 適している点

1. **接続数**: 100人ならFree Planでも十分（200接続制限）
2. **低遅延**: Broadcastは一時的メッセージでDB経由なし
3. **リアルタイム性**: WebSocketベースで高性能
4. **技術的信頼性**: Elixir/Phoenixの並行処理に強み

### ⚠️ 懸念点・制約

1. **メッセージ配信保証なし**
   - 重要なゲーム状態は別途DBに保存必要
   - タイピング進捗の一部ロストの可能性

2. **Presence機能の計算負荷**
   - 100人の状態同期は計算集約的
   - 公式でも「控えめに使用」推奨

3. **高頻度更新での課題**
   - タイピング速度によってはメッセージ量が急増
   - 月間制限への影響

## 推奨アーキテクチャ設計 🏗️

### 1. ハイブリッド設計
```
- Broadcast: キーストローク、リアルタイム表示
- Presence: 参加者状態、「誰が入力中」表示
- Postgres Changes: ゲーム結果、重要な状態変更
```

### 2. 負荷分散戦略
```
- 部屋ごとにChannelを分離
- 不要なメッセージをフィルタリング
- Presenceの更新頻度を制御（スロットリング）
```

### 3. フォールバック設計
```
- 重要なデータは必ずDB永続化
- 接続断での状態復旧機能
- メッセージロスト時の再同期機能
```

## TYPE 2 LIVEでの実装推奨 🎮

### メッセージ設計
```typescript
// 軽量なBroadcastメッセージ
interface TypingEvent {
  userId: string;
  progress: number;  // 0-100%
  timestamp: number;
}

// Presenceでの状態管理
interface UserPresence {
  userId: string;
  status: 'playing' | 'finished' | 'disconnected';
  lastSeen: number;
}
```

### 負荷軽減のための工夫
1. **デバウンス**: キーストロークを100ms間隔で送信
2. **差分更新**: 進捗の変化時のみ送信
3. **Channel分離**: ゲーム部屋ごとに独立したチャンネル

## 結論 📋

### 🟢 適性: 十分に可能
- 100人規模なら技術的に実現可能
- 適切な設計により高いユーザー体験を提供

### 🟡 条件付き推奨
- メッセージ配信保証なしを前提とした設計
- 重要データの永続化は必須
- 適切な負荷分散とスロットリング

### 🔴 注意点
- 急激なスケール（300人超）では再検討必要
- メッセージ量の継続的な監視が必要
- 代替手段（WebRTC、専用サーバー）の検討も視野に

## 次のステップ 🎯
1. プロトタイプでの負荷テスト実施
2. メッセージ量の実測値取得
3. 段階的スケーリング計画の策定
